{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ELEC 474\n",
    "Matthieu Roux\n",
    "Student Number: 20013052\n",
    "\n",
    "## Importing modules and images\n",
    "\n",
    "Importing modules, images, and creating helper methods\n",
    "\n",
    "- `show_img` displays an image in an open cv window\n",
    "- `make_red` recolors an image by changing its blue elments ot red, in the cazse of cereal, it creates a realistic red cereal box (instead of a blue one)\n",
    "- `is_balck` checks if a pixel is black\n",
    "- `apply_overlay` applies a color overlay on a grey image"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from numpy.core.fromnumeric import size\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "# Reference image import\n",
    "cereal_url = \"cereal.jpg\"\n",
    "\n",
    "img_ref_bgr = cv2.imread(cereal_url)\n",
    "img_ref = cv2.cvtColor(img_ref_bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Perspective import\n",
    "cereal_r_url = \"cereal_r.jpg\"\n",
    "cereal_l_url = \"cereal_l.jpg\"\n",
    "cereal_tr_url = \"cereal_tr.jpg\"\n",
    "cereal_tl_url = \"cereal_tl.jpg\"\n",
    "cereal_per_url = cereal_tl_url\n",
    "\n",
    "cereal_r = cv2.cvtColor(cv2.imread(cereal_r_url), cv2.COLOR_BGR2GRAY)\n",
    "cereal_l = cv2.cvtColor(cv2.imread(cereal_l_url), cv2.COLOR_BGR2GRAY)\n",
    "cereal_tr = cv2.cvtColor(cv2.imread(cereal_tr_url), cv2.COLOR_BGR2GRAY)\n",
    "cereal_tl = cv2.cvtColor(cv2.imread(cereal_tl_url), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def show_img(img, name=\"my image\"):\n",
    "    cv2.namedWindow(name)\n",
    "    while True:\n",
    "        # Wait a little bit for the image to re-draw\n",
    "        key = cv2.waitKey(5)\n",
    "        cv2.imshow(name, img)\n",
    "\n",
    "        # If an x is pressed, the window will close\n",
    "        if key == ord(\"x\"):\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def make_red(img):\n",
    "    img_red = deepcopy(img)\n",
    "    for index in np.ndindex(img_red.shape[:2]):\n",
    "        # if an image is too blue and not red enough...\n",
    "        if img_red[index][0] > img_red[index][2] * 1.25:\n",
    "            # ...swap the red and blue values!\n",
    "            temp = img_red[index][0]\n",
    "            img_red[index][0] = img_red[index][2]\n",
    "            img_red[index][2] = temp\n",
    "    return img_red\n",
    "\n",
    "    \n",
    "def is_black(pixel):\n",
    "    for sub_px in pixel:\n",
    "        if sub_px > 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def apply_overlay(background, overlay):\n",
    "    img_overlaid = cv2.cvtColor(background, cv2.COLOR_GRAY2BGR)\n",
    "    for index in np.ndindex(img_overlaid.shape[:2]):\n",
    "        try:\n",
    "            if not is_black(overlay[index]):\n",
    "                img_overlaid[index] = overlay[index]\n",
    "        except:\n",
    "            pass\n",
    "    return img_overlaid\n"
   ]
  },
  {
   "source": [
    "## 1.1 Matching"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowe_ratio_match(matches, threshold_ratio=0.7):\n",
    "    return [m for m, n in matches if m.distance < threshold_ratio * n.distance]\n",
    "\n",
    "def get_matches(img_1, img_2):\n",
    "    sift = cv2.SIFT()\n",
    "    # putting keypoints in variabels for better legibility\n",
    "    kp_img_1, des_img_1 = sift.detectAndCompute(img_1, None)\n",
    "    kp_img_2, des_img_2 = sift.detectAndCompute(img_2, None)\n",
    "\n",
    "    matches = compute_matches(des_img_1, des_img_2)\n",
    "    lowe_ratio_matches = lowe_ratio_match(matches)\n",
    "    return (kp_img_1, des_img_1), (kp_img_2, des_img_2), lowe_ratio_matches\n",
    "\n",
    "\n",
    "def get_matches(img_1, img_2):\n",
    "    my_SIFT_instance = cv2.SIFT_create()\n",
    "    # putting keypoints in variabels for better legibility\n",
    "    kp1, des1 = my_SIFT_instance.detectAndCompute(img_1, None)\n",
    "    kp2, des2 = my_SIFT_instance.detectAndCompute(img_2, None)\n",
    "\n",
    "    # matching\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    lowe_matches = lowe_ratio_match(matches)\n",
    "    return (kp1, des1), (kp2, des2), lowe_matches"
   ]
  },
  {
   "source": [
    "## 1.2 Affine Transform"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_transform(img, rotation_value=42, scaling_value=0.4):\n",
    "    height, width = img.shape\n",
    "    center_index = (int(height / 2), int(width / 2))\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(\n",
    "        center_index, rotation_value, scaling_value\n",
    "    )\n",
    "    new_img = cv2.warpAffine(img, rotation_matrix, (height, width))\n",
    "    return new_img\n",
    "\n",
    "# generate an affine image\n",
    "img_affine = affine_transform(img_ref)\n",
    "\n",
    "# obtain matches and keypoints\n",
    "ref_params, affine_params, lowe_matches = get_matches(img_ref, img_affine)\n",
    "\n",
    "kp_ref, des_ref = ref_params\n",
    "kp_affine, des_affine = affine_params\n",
    "\n",
    "# format points\n",
    "ref_pts = np.float32(\n",
    "    [kp_ref[m.queryIdx].pt for m in lowe_matches],\n",
    ").reshape(-1, 1, 2)\n",
    "img_pts = np.float32(\n",
    "    [kp_affine[m.trainIdx].pt for m in lowe_matches],\n",
    ").reshape(-1, 1, 2)\n",
    "\n",
    "estimated_rotation_matrix = cv2.estimateAffinePartial2D(ref_pts, img_pts)[0]\n",
    "\n",
    "modified_img = make_red(img_ref_bgr)\n",
    "affine_overlay = cv2.warpAffine(\n",
    "    modified_img,\n",
    "    estimated_rotation_matrix,\n",
    "    (modified_img.shape[0], modified_img.shape[1]),\n",
    ")\n",
    "img_affine_overlaid = apply_overlay(background=img_affine, overlay=affine_overlay)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## 1.3 Perspective"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspective_overlay(\n",
    "    img_ref, img_perspective, modified_img, name=\"my perspectie test\"\n",
    "):\n",
    "    # obtain matches and keypoints\n",
    "    ref_params, affine_params, lowe_matches = get_matches(img_ref, img_perspective)\n",
    "\n",
    "    kp_ref = ref_params[0]\n",
    "    kp_perspective = affine_params[0]\n",
    "\n",
    "    # format points\n",
    "    ref_pts = np.float32(\n",
    "        [kp_ref[m.queryIdx].pt for m in lowe_matches],\n",
    "    ).reshape(-1, 1, 2)\n",
    "    img_pts = np.float32(\n",
    "        [kp_perspective[m.trainIdx].pt for m in lowe_matches],\n",
    "    ).reshape(-1, 1, 2)\n",
    "\n",
    "    homography_matrix = cv2.findHomography(ref_pts, img_pts, cv2.RANSAC)[0]\n",
    "\n",
    "    img_overlay = cv2.warpPerspective(\n",
    "        modified_img,\n",
    "        homography_matrix,\n",
    "        (modified_img.shape[0], modified_img.shape[1]),\n",
    "    )\n",
    "    img_per_overlaid = apply_overlay(background=img_perspective, overlay=img_overlay)\n",
    "\n",
    "    show_img(img_per_overlaid, name=\"my perspectie test\")"
   ]
  },
  {
   "source": [
    "## 2. Runnining and displaying\n",
    "\n",
    "Press **x** to close the window and move on to the next one, the order of display of images is:\n",
    "1. Reference image (graysacle)\n",
    "2. Affine before overlay\n",
    "3. Affine after overlay\n",
    "4. All perspective images with their original grayscale image, followed by their overlay counterparts. **overlay may take a bit to load**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying\n",
    "show_img(cv2.cvtColor(img_ref, cv2.COLOR_GRAY2BGR), name=\"reference\")\n",
    "\n",
    "# Display affine images, original then overlay\n",
    "show_img(cv2.cvtColor(img_affine, cv2.COLOR_GRAY2BGR), name=\"affine_original\")\n",
    "show_img(img_affine_overlaid, name=\"affine_overlaid\")\n",
    "\n",
    "# Display perspective images, original then overlay\n",
    "show_img(cv2.cvtColor(cereal_r, cv2.COLOR_GRAY2BGR), name=\"cereal_r_original\")\n",
    "perspective_overlay(img_ref, cereal_r, modified_img, name=\"cereal_r_overlaid\")\n",
    "\n",
    "show_img(cv2.cvtColor(cereal_l, cv2.COLOR_GRAY2BGR), name=\"cereal_l_original\")\n",
    "perspective_overlay(img_ref, cereal_l, modified_img, name=\"cereal_l_overlaid\")\n",
    "\n",
    "show_img(cv2.cvtColor(cereal_tr, cv2.COLOR_GRAY2BGR), name=\"cereal_tr_original\")\n",
    "perspective_overlay(img_ref, cereal_tr, modified_img, name=\"cereal_tr_overlaid\")\n",
    "\n",
    "show_img(cv2.cvtColor(cereal_tl, cv2.COLOR_GRAY2BGR), name=\"cereal_tl_original\")\n",
    "perspective_overlay(img_ref, cereal_tl, modified_img, name=\"cereal_tl_overlaid\")"
   ]
  }
 ]
}